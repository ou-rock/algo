# 强化学习入门示例

这是一个最简单的强化学习示例，包含一个简单的环境和智能体，帮助理解强化学习的基本概念。

## 📚 什么是强化学习？

强化学习（Reinforcement Learning, RL）是机器学习的一个分支，通过**智能体**（Agent）与**环境**（Environment）交互来学习最优策略。

### 核心概念

- **智能体（Agent）**：做出决策的主体
- **环境（Environment）**：智能体所处的世界
- **状态（State）**：环境的当前情况
- **动作（Action）**：智能体可以执行的操作
- **奖励（Reward）**：环境对智能体动作的反馈
- **策略（Policy）**：从状态到动作的映射

### 交互流程

```
智能体观察状态 → 选择动作 → 执行动作 → 获得奖励和新状态 → 学习 → 循环
```

## 📁 文件说明

### 1. `simple_environment.py` - 简单的网格世界环境

一个 4×4 的网格世界：
- **起点**：左上角 (0, 0)
- **终点**：右下角 (3, 3)
- **动作**：上、下、左、右
- **奖励规则**：
  - 到达终点：+10
  - 每移动一步：-1

```python
from simple_environment import GridWorld

env = GridWorld(size=4)
state = env.reset()
env.render()
```

### 2. `simple_agent.py` - Q-learning 智能体

实现了两种智能体：

#### QLearningAgent - Q学习智能体

使用 Q-learning 算法学习最优策略：

**Q-learning 更新公式**：
```
Q(s,a) ← Q(s,a) + α[r + γ·max Q(s',a') - Q(s,a)]
```

**参数说明**：
- `α` (learning_rate)：学习率，控制新信息的接受程度
- `γ` (discount_factor)：折扣因子，控制未来奖励的重要性
- `ε` (epsilon)：探索率，控制随机探索的概率

```python
from simple_agent import QLearningAgent

agent = QLearningAgent(
    state_space_size=16,
    action_space_size=4,
    learning_rate=0.1,
    discount_factor=0.9,
    epsilon=0.1
)
```

#### RandomAgent - 随机智能体

总是随机选择动作，用于对比基准。

### 3. `rl_demo.py` - 完整演示程序

演示了强化学习的完整流程：
1. 创建环境和智能体
2. 训练前测试（随机策略）
3. 训练智能体
4. 训练后测试
5. 可视化学到的策略
6. 演示一个完整回合
7. 绘制训练曲线

## 🚀 快速开始

### 基本要求

```bash
# Python 3.6+
pip install numpy matplotlib
```

### 运行演示

```bash
# 运行完整演示
python rl_demo.py

# 测试环境
python simple_environment.py

# 测试智能体
python simple_agent.py
```

### 示例输出

```
强化学习演示：Q-learning 在网格世界中学习
==================================================

1. 创建环境
--------------------------------------------------
环境：4x4 网格世界
起点：(0, 0)
终点：(3, 3)

2. 创建智能体
--------------------------------------------------
智能体：Q-learning
学习率 α = 0.1
折扣因子 γ = 0.9
探索率 ε = 0.1

3. 训练前测试（随机策略）
--------------------------------------------------
成功率: 5.0%
平均步数: 94.2

4. 训练智能体
--------------------------------------------------
回合 100/500, 平均奖励: -15.23, 平均步数: 25.23
回合 200/500, 平均奖励: -8.45, 平均步数: 18.45
回合 300/500, 平均奖励: -6.12, 平均步数: 16.12
回合 400/500, 平均奖励: -5.34, 平均步数: 15.34
回合 500/500, 平均奖励: -5.01, 平均步数: 15.01

5. 训练后测试
--------------------------------------------------
成功率: 98.0%
平均步数: 6.2

6. 学到的策略
--------------------------------------------------
=================
| → | → | → | ↓ |
=================
| ↓ | ↓ | ↓ | ↓ |
=================
| ↓ | → | → | ↓ |
=================
| → | → | → | G |
=================
```

## 🎯 学习要点

### 1. 强化学习的基本流程

```python
# 初始化
env = GridWorld(size=4)
agent = QLearningAgent(...)

# 训练循环
for episode in range(num_episodes):
    state = env.reset()

    while not done:
        # 1. 选择动作
        action = agent.choose_action(state)

        # 2. 执行动作
        next_state, reward, done, info = env.step(action)

        # 3. 学习
        agent.learn(state, action, reward, next_state, done)

        state = next_state
```

### 2. 探索与利用的权衡

- **探索（Exploration）**：尝试新的动作，发现更好的策略
- **利用（Exploitation）**：使用已知的最佳动作，获得更多奖励

ε-贪心策略：
- 以 ε 的概率随机探索
- 以 1-ε 的概率选择最佳动作

### 3. Q-learning 的优势

- **无模型（Model-free）**：不需要知道环境的动态模型
- **离策略（Off-policy）**：可以从其他策略生成的数据中学习
- **简单有效**：易于实现，适合入门学习

## 🔧 参数调优建议

### 学习率 (α)
- **较大值** (0.5-1.0)：学习快，但可能不稳定
- **中等值** (0.1-0.3)：平衡学习速度和稳定性
- **较小值** (0.01-0.05)：学习慢，但更稳定

### 折扣因子 (γ)
- **接近1** (0.9-0.99)：重视长期奖励
- **中等值** (0.5-0.8)：平衡短期和长期
- **接近0** (0.1-0.3)：只关注即时奖励

### 探索率 (ε)
- **训练初期**：较大值 (0.3-0.5) 鼓励探索
- **训练中期**：中等值 (0.1-0.2) 平衡探索和利用
- **训练后期**：较小值 (0.01-0.05) 主要利用已学策略

## 📊 扩展练习

1. **修改环境**：
   - 增加网格大小
   - 添加障碍物
   - 设置多个终点
   - 改变奖励结构

2. **改进智能体**：
   - 实现衰减的探索率
   - 实现 SARSA 算法
   - 添加经验回放

3. **可视化**：
   - 绘制 Q 值热图
   - 动画演示学习过程
   - 对比不同参数的效果

## 📖 延伸阅读

- [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html) - Sutton & Barto
- [OpenAI Gym](https://gym.openai.com/) - 标准强化学习环境库
- [Deep Reinforcement Learning](https://spinningup.openai.com/) - OpenAI Spinning Up

## 💡 常见问题

### Q: 为什么智能体不学习？
A: 检查学习率是否过小、奖励设计是否合理、训练回合数是否足够。

### Q: 如何知道训练是否成功？
A: 观察训练曲线是否收敛、测试成功率是否提高、平均步数是否减少。

### Q: Q-learning 和深度 Q 网络（DQN）的区别？
A: Q-learning 使用表格存储 Q 值，适合小规模问题；DQN 使用神经网络逼近 Q 函数，适合大规模问题。

---

**作者**：Claude
**日期**：2026-01-01
**版本**：1.0
